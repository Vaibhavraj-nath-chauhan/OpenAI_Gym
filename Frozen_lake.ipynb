{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                              #importing numpy\n",
    "import gym                                      #importing gym\n",
    "import random                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")                           #creating a enviroment of frozenlake\n",
    "action_space_size = env.action_space.n                    #fatching number of action\n",
    "state_space_size = env.observation_space.n                #fetching number of observations (l,r,u,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S F F F                                          \n",
      "F H F H       \n",
      "F F F H       \n",
      "H F F G\n",
      "\n",
      "\n",
      "Here S for start\n",
      "G for goal\n",
      "F for frozen surface (safe)\n",
      "H for hole  (lose)\n"
     ]
    }
   ],
   "source": [
    "#game pattren\n",
    "print(\"\"\"S F F F                                          \n",
    "F H F H       \n",
    "F F F H       \n",
    "H F F G\n",
    "\n",
    "\n",
    "Here S for start\n",
    "G for goal\n",
    "F for frozen surface (safe)\n",
    "H for hole  (lose)\"\"\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16\n"
     ]
    }
   ],
   "source": [
    "print(action_space_size,state_space_size)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space_size,action_space_size))   #creating an array for state and action carring zero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000                            #number of eposides we want to run\n",
    "max_steps_per_episode = 100                     #number of steps given to model under every eposide\n",
    " \n",
    "learning_rate = 0.1                             #setting learning rate indication at which rate our machine will learn for better understanding\n",
    "discount_rate = .99                             #\n",
    "\n",
    "exploration_rate = 1                            #its indicating our machine sholud explore or work accoung to last updated value based omn reward\n",
    "max_exploration_rate = 1                        #max rate of explroing\n",
    "min_exploration_rate = 0.01                     #min rate of exploring\n",
    "exploration_decay_rate = 0.001                  #its showing how our exloatation rate will decrease\n",
    "#based of max and min and exploration decay rate we are going to decreasre the exploration rate and increase exploitation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes  = []                              #used to collect rewards of every episodes\n",
    "for episode in range(num_episodes):                     #loop\n",
    "    state = env.reset()                                 #at every episode we will set our value to start point\n",
    "    \n",
    "    done = False                                        #setting by default to zero\n",
    "    rewards_current_episode = 0                         #used to collect the reward of current or running episode\n",
    "    for step in range(max_steps_per_episode):                     #running the second loop defining each step\n",
    "            exploration_rate_threshold = random.uniform(0,1)      #fetching 0 or 1 it will set that our machine chose exploration or exploitation\n",
    "            if exploration_rate_threshold > exploration_rate:     \n",
    "                action = np.argmax(q_table[state,:])    #here machine choose exploation means finding the best output from earlies rewards\n",
    "            else:\n",
    "                action = env.action_space.sample()      #here maching chosing random action to perfrom task\n",
    "            \n",
    "            #updating Q table\n",
    "            new_state , reward , done , info = env.step(action)   #passing our action to enviroment \n",
    "            #basend on our acton step function returning new_state, float value for reward,boolen for done and dict for info\n",
    "            \n",
    "            # Update Q-table for Q(s,a)\n",
    "            q_table[state,action] = q_table[state,action ]*(1-learning_rate)+learning_rate*(reward+discount_rate*np.max(q_table[new_state,:]))\n",
    "            #this is the main formula of q-learning here we are updating our q_table for every possible state and action perform on that state\n",
    "\n",
    "            state = new_state                  #here we are updating our updating our state returend from step function\n",
    "            rewards_current_episode +=reward\n",
    "            \n",
    "            if done ==True:                    #if our boolen siad that its done no matter win or lose our loop will break\n",
    "                break\n",
    "                \n",
    "    exploration_rate = min_exploration_rate+ (max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay_rate*episode)\n",
    "    # ^^^ here we are decreasing the value of explorating rate  \n",
    "    rewards_all_episodes.append(rewards_current_episode)   #colecting all rewards value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 : 0.04100000000000003\n",
      "2000 : 0.19500000000000015\n",
      "3000 : 0.4020000000000003\n",
      "4000 : 0.5490000000000004\n",
      "5000 : 0.5950000000000004\n",
      "6000 : 0.6410000000000005\n",
      "7000 : 0.6610000000000005\n",
      "8000 : 0.6710000000000005\n",
      "9000 : 0.6740000000000005\n",
      "10000 : 0.7090000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "for r in reqards_per_thosand_episodes:\n",
    "    print(count,\":\",str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.58104323 0.52900305 0.52779682 0.53213348]\n",
      " [0.2227359  0.26436896 0.24627962 0.52182634]\n",
      " [0.44158646 0.42014665 0.42035078 0.48564527]\n",
      " [0.36199664 0.40095372 0.3482984  0.45264556]\n",
      " [0.59818386 0.4602776  0.48031464 0.28448503]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17812292 0.08967009 0.2648899  0.11247258]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39112692 0.57413149 0.36916847 0.62862632]\n",
      " [0.4251476  0.69143331 0.43395991 0.34975388]\n",
      " [0.63897002 0.43825507 0.36591811 0.22172545]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39014788 0.48969266 0.75226653 0.39944891]\n",
      " [0.73975008 0.878356   0.74955106 0.77237422]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
